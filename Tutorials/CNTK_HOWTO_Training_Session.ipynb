{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK: Manual minibatch loop and Training Session\n",
    " \n",
    "This tutorial demonstrates a canonical training minibatch loop and how it can be rewritten using a training session. We discuss what the main caveats are and how they can be avoided.\n",
    "\n",
    "__Note: Please consider using a higher-level [Function.train](https://www.cntk.ai/pythondocs/cntk.ops.functions.html#cntk.ops.functions.Function.train)/[Function.test](https://www.cntk.ai/pythondocs/cntk.ops.functions.html#cntk.ops.functions.Function.test) functionality instead of directly using the training session API. For more information please see [this tutorial](https://github.com/Microsoft/CNTK/blob/v2.0/Tutorials/CNTK_200_GuidedTour.ipynb).__\n",
    "\n",
    "## Manual training loop\n",
    "\n",
    "Many scripts in CNTK have a very similar structure:\n",
    " - they create a network\n",
    " - instantiate a trainer and a learner with appropriate hyper-parameters\n",
    " - load training and testing data with minibatch sources\n",
    " - then run the main training loop fetching the data from the train minibatch source and feeding it to the trainer for N samples/sweeps\n",
    " - at the end they perform the eval loop using data from the test minibatch source and calling [test_minibatch](https://www.cntk.ai/pythondocs/cntk.train.trainer.html?highlight=test_minibatch#cntk.train.trainer.Trainer.test_minibatch) on the trainer or evaluator\n",
    "\n",
    "As an example for such a script we will take a toy task of learning XOR operation with a simple feed forward network.\n",
    "We will try to learn the following function:\n",
    "\n",
    "|x|y|result|\n",
    "|:-|:-|:------|\n",
    "|0|0|   0  |\n",
    "|0|1|   1  |\n",
    "|1|0|   1  |\n",
    "|0|0|   0  |\n",
    "\n",
    "The network will have two dense layers, we use [tanh](https://www.cntk.ai/pythondocs/cntk.ops.html?highlight=tanh#cntk.ops.tanh) as an activation for the first layer and no activation for the second. The sample script is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per sample: 0.1\n",
      " Minibatch[   1-  10]: loss = 0.704373 * 40, metric = 70.44% * 40;\n",
      " Minibatch[  11-  20]: loss = 0.249296 * 40, metric = 24.93% * 40;\n",
      " Minibatch[  21-  30]: loss = 0.247031 * 40, metric = 24.70% * 40;\n",
      " Minibatch[  31-  40]: loss = 0.240489 * 40, metric = 24.05% * 40;\n",
      " Minibatch[  41-  50]: loss = 0.222190 * 40, metric = 22.22% * 40;\n",
      " Minibatch[  51-  60]: loss = 0.185867 * 40, metric = 18.59% * 40;\n",
      " Minibatch[  61-  70]: loss = 0.135534 * 40, metric = 13.55% * 40;\n",
      " Minibatch[  71-  80]: loss = 0.081844 * 40, metric = 8.18% * 40;\n",
      " Minibatch[  81-  90]: loss = 0.038817 * 40, metric = 3.88% * 40;\n",
      " Minibatch[  91- 100]: loss = 0.014302 * 40, metric = 1.43% * 40;\n",
      " Minibatch[ 101- 110]: loss = 0.004291 * 40, metric = 0.43% * 40;\n",
      " Minibatch[ 111- 120]: loss = 0.001349 * 40, metric = 0.13% * 40;\n",
      " Minibatch[ 121- 130]: loss = 0.326946 * 40, metric = 32.69% * 40;\n",
      " Minibatch[ 131- 140]: loss = 0.036077 * 40, metric = 3.61% * 40;\n",
      " Minibatch[ 141- 150]: loss = 0.002290 * 40, metric = 0.23% * 40;\n",
      " Minibatch[ 151- 160]: loss = 0.000401 * 40, metric = 0.04% * 40;\n",
      " Minibatch[ 161- 170]: loss = 0.000073 * 40, metric = 0.01% * 40;\n",
      " Minibatch[ 171- 180]: loss = 0.000014 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 181- 190]: loss = 0.000003 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 191- 200]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      "Metric= 0.000000\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import cntk\n",
    "import cntk.ops\n",
    "import cntk.io\n",
    "import cntk.train\n",
    "\n",
    "from cntk.layers import Dense, Sequential\n",
    "from cntk.io import StreamDef, StreamDefs, MinibatchSource, CTFDeserializer\n",
    "from cntk.logging import ProgressPrinter\n",
    "\n",
    "# Let's prepare data in the CTF format. It exactly matches\n",
    "# the table above\n",
    "INPUT_DATA = r'''|xy 0 0\t|r 0\n",
    "|xy 1 0\t|r 1\n",
    "|xy 0 1\t|r 1\n",
    "|xy 1 1\t|r 0\n",
    "'''\n",
    "\n",
    "# Write the data to a temporary file\n",
    "input_file = 'input'\n",
    "with open(input_file, 'w') as f:\n",
    "    f.write(INPUT_DATA)\n",
    "\n",
    "# Create a network\n",
    "xy = cntk.input_variable(2)\n",
    "label = cntk.input_variable(1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2, activation=cntk.ops.tanh),\n",
    "    Dense(1)])\n",
    "\n",
    "z = model(xy)\n",
    "loss = cntk.squared_error(z, label)\n",
    "\n",
    "# Define our input data streams\n",
    "streams = StreamDefs(\n",
    "    xy = StreamDef(field='xy', shape=2),\n",
    "    r = StreamDef(field='r', shape=1))\n",
    "\n",
    "# Create a learner and a trainer and a progress writer to \n",
    "# output current progress\n",
    "learner = cntk.sgd(model.parameters, cntk.learning_rate_schedule(0.1, cntk.UnitType.sample))\n",
    "trainer = cntk.train.Trainer(z, (loss, loss), learner, ProgressPrinter(freq=10))\n",
    "\n",
    "# Now let's create a minibatch source for out input file\n",
    "mb_source = MinibatchSource(CTFDeserializer(input_file, streams))\n",
    "input_map = { xy : mb_source['xy'], label : mb_source['r'] }\n",
    "\n",
    "# Run a manual training minibatch loop\n",
    "minibatch_size = 4\n",
    "max_samples = 800\n",
    "train = True\n",
    "while train and trainer.total_number_of_samples_seen < max_samples:\n",
    "    data = mb_source.next_minibatch(minibatch_size, input_map)\n",
    "    train = trainer.train_minibatch(data)\n",
    "\n",
    "# Run a manual evaluation loop ussing the same data file for evaluation\n",
    "test_mb_source = MinibatchSource(CTFDeserializer(input_file, streams), randomize=False, max_samples=100)\n",
    "test_input_map = { xy : test_mb_source['xy'], label : test_mb_source['r'] }\n",
    "total_samples = 0\n",
    "error = 0.\n",
    "data = test_mb_source.next_minibatch(32, input_map)\n",
    "while data:\n",
    "    total_samples += data[label].number_of_samples \n",
    "    error += trainer.test_minibatch(data) * data[label].number_of_samples\n",
    "    data = test_mb_source.next_minibatch(32, test_input_map)\n",
    "\n",
    "print(\"Metric= %f\" % (error / total_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As it can be seen above, the actual model is specified in just two lines, the rest is a boilerplate code to iterate over the data and feed it manually for training and evaluation. With a manual loop, the user has the complete flexibility how to feed the data, but she also has to take several not so obvious things into account.\n",
    "\n",
    "For simplicity we use a toy example, but imaging a situation when your job runs for a couple of days.\n",
    "\n",
    "### Failover and recovery\n",
    "\n",
    "For the small sample above the recovery is not important, but in case the training spans several weeks or days it is not safe to assume that the machine stays online all the time and there are no hardware or software glitches. If the machine reboots, goes down or the script has a bug the user will have to rerun the same experiment from the beginning. That is highly undesirable. To avoid that CNTK allows the user to perform checkpoints and restore from them in the event of failure.\n",
    "\n",
    "One of the means to save the model state in CNTK is by using [save method](https://cntk.ai/pythondocs/cntk.ops.functions.html?highlight=save#cntk.ops.functions.Function.save) on the [Function class](https://cntk.ai/pythondocs/cntk.ops.functions.html?highlight=save#cntk.ops.functions.Function).\n",
    "It is worth mentioning that this function only saves the model state, but there are other stateful entities in the script, including:\n",
    " * minibatch sources\n",
    " * trainer\n",
    " * learners\n",
    " \n",
    "In order to save the complete state of the script, the user has to manually save the current state of the minibatch source and the trainer. The minibatch source provides [get_checkpoint_state](https://www.cntk.ai/pythondocs/cntk.io.html?highlight=get_checkpoint_state#cntk.io.MinibatchSource.get_checkpoint_state) method, the result can be passed to the trainer [save_checkpoint](https://www.cntk.ai/pythondocs/cntk.train.trainer.html?highlight=save_checkpoint#cntk.train.trainer.Trainer.save_checkpoint) method, that takes care of saving the state to disk or exchanging the state in case of distributed training. There are also the corresponding [restore_from_checkpoint](https://www.cntk.ai/pythondocs/cntk.train.trainer.html?highlight=restore_from_checkpoint#cntk.train.trainer.Trainer.restore_from_checkpoint) methods on the trainer and the minibatch source that can be used for restore. To recover from error, on start up the user has to restore a state using the trainer and set the current position of the minibatch source.\n",
    "\n",
    "With the above in mind, let's rewrite our loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore from checkpoint\n",
      "Restore has finished successfully\n"
     ]
    }
   ],
   "source": [
    "# Run a manual training minibatch loop with checkpointing\n",
    "import os\n",
    "\n",
    "# Initialize main objects\n",
    "mb_source = MinibatchSource(CTFDeserializer(input_file, streams))\n",
    "input_map = { xy : mb_source['xy'], label : mb_source['r'] }\n",
    "\n",
    "learner = cntk.sgd(model.parameters, cntk.learning_rate_schedule(0.1, cntk.UnitType.sample))\n",
    "trainer = cntk.train.Trainer(z, (loss, loss), learner, ProgressPrinter(freq=10))\n",
    "\n",
    "# Try to restore if the checkpoint exists\n",
    "checkpoint = 'manual_loop_checkpointed'\n",
    "\n",
    "if os.path.exists(checkpoint):\n",
    "    print(\"Trying to restore from checkpoint\")\n",
    "    mb_source_state = trainer.restore_from_checkpoint(checkpoint)\n",
    "    mb_source.restore_from_checkpoint(mb_source_state)\n",
    "    print(\"Restore has finished successfully\")\n",
    "else:\n",
    "    print(\"No restore file found\")\n",
    "    \n",
    "checkpoint_frequency = 100\n",
    "last_checkpoint = 0\n",
    "train = True\n",
    "while train and trainer.total_number_of_samples_seen < max_samples:\n",
    "    data = mb_source.next_minibatch(minibatch_size, input_map)\n",
    "    train = trainer.train_minibatch(data)\n",
    "    if trainer.total_number_of_samples_seen / checkpoint_frequency != last_checkpoint:\n",
    "        mb_source_state = mb_source.get_checkpoint_state()\n",
    "        trainer.save_checkpoint(checkpoint, mb_source_state)\n",
    "        last_checkpoint = trainer.total_number_of_samples_seen / checkpoint_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At the beginning we check if the checkpoint file exists and we can restore from it. After that we start the training. Our loop is based on the total number of samples the trainer has seen. This information is included in the checkpoint, so in \n",
    "case of failure the training will resume at the saved position (this will become even more important for distributed training).\n",
    "\n",
    "Depending on the checkpointing frequency the above script retrieves the current state of the minibatch source and creates a checkpoint using the trainer. If the script iterates over the same data many times, saving the state of the minibatch source is not that important, but for huge workloads you probably do not want to start seeing the same data from the beginning.\n",
    "\n",
    "At some point the user will want to parallelize the script to decrease the training time. Let's look how this can be done in the next section.\n",
    "\n",
    "### Distributed manual loop\n",
    "\n",
    "In order to make training distributed CNTK provides a set of distributed learner that encapsulate a set of algorithms (1BitSGD, BlockMomentum, data parallel SGD) that uses MPI to exchage the state. From the script perspecitve, almost everything stays the same. The only difference is that the user needs to wrap the learner into the corresponding distributed learner and make sure she picks up the data from the minibatch source based on the current worker rank (also the script should be run with ```mpiexec```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No restore file found\n",
      " Minibatch[   1-  10]: loss = 0.000031 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  11-  20]: loss = 0.000019 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  21-  30]: loss = 0.000011 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  31-  40]: loss = 0.000007 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  41-  50]: loss = 0.000004 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  51-  60]: loss = 0.000003 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  61-  70]: loss = 0.000002 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  71-  80]: loss = 0.000001 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  81-  90]: loss = 0.000001 * 40, metric = 0.00% * 40;\n",
      " Minibatch[  91- 100]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 101- 110]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 111- 120]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 121- 130]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 131- 140]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 141- 150]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 151- 160]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 161- 170]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 171- 180]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 181- 190]: loss = 0.000000 * 40, metric = 0.00% * 40;\n",
      " Minibatch[ 191- 200]: loss = 0.000000 * 40, metric = 0.00% * 40;\n"
     ]
    }
   ],
   "source": [
    "# Run a manual training minibatch loop with distributed learner\n",
    "checkpoint = 'manual_loop_distributed'\n",
    "\n",
    "mb_source = MinibatchSource(CTFDeserializer(input_file, streams))\n",
    "input_map = { xy : mb_source['xy'], label : mb_source['r'] }\n",
    "\n",
    "# Make sure the learner is distributed\n",
    "distributed_learner = cntk.distributed.data_parallel_distributed_learner(\n",
    "    cntk.sgd(model.parameters, cntk.learning_rate_schedule(0.1, cntk.UnitType.sample)))\n",
    "trainer = cntk.train.Trainer(z, (loss, loss), distributed_learner, ProgressPrinter(freq=10))\n",
    "\n",
    "if os.path.exists(checkpoint):\n",
    "    print(\"Trying to restore from checkpoint\")\n",
    "    mb_source_state = trainer.restore_from_checkpoint(checkpoint)\n",
    "    mb_source.restore_from_checkpoint(mb_source_state)\n",
    "else:\n",
    "    print(\"No restore file found\")\n",
    "\n",
    "last_checkpoint = 0\n",
    "train = True\n",
    "partition = cntk.distributed.Communicator.rank()\n",
    "num_partitions = cntk.distributed.Communicator.num_workers()\n",
    "while train and trainer.total_number_of_samples_seen < max_samples:\n",
    "    # Make sure each worker gets its own data only\n",
    "    data = mb_source.next_minibatch(minibatch_size_in_samples = minibatch_size,\n",
    "                                    input_map = input_map, device = cntk.use_default_device(), \n",
    "                                    num_data_partitions=num_partitions, partition_index=partition)\n",
    "    train = trainer.train_minibatch(data)\n",
    "    if trainer.total_number_of_samples_seen / checkpoint_frequency != last_checkpoint:\n",
    "        mb_source_state = mb_source.get_checkpoint_state()\n",
    "        trainer.save_checkpoint(checkpoint, mb_source_state)\n",
    "        last_checkpoint = trainer.total_number_of_samples_seen / checkpoint_frequency\n",
    "\n",
    "# When you use distributed learners, please call finalize MPI at the end of your script, \n",
    "# see the next cell.\n",
    "# cntk.distributed.Communicator.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for distribution to work properly, the minibatch loop should exit by all workers at the same time. Some of the workers can have more data then the others, so the exit condition of the loop should be based on the return value of the trainer (if no more work should be done by a particular worker this can be communicated by passing an empty minibatch to ```train_minibatch```).\n",
    "\n",
    "As has been noted before, the decisions inside the loop are based on the [Trainer.total_number_of_samples_seen](https://www.cntk.ai/pythondocs/cntk.train.trainer.html?highlight=restore_from_checkpoint#cntk.train.trainer.Trainer.total_number_of_samples_seen). Some of the operations (i.e. ```train_minibatch```, checkpoint, cross validation, if done in a distributed fashion) require synchronization and to match among all the workers they use a global state - the global number of samples seen by the trainer.\n",
    "\n",
    "Even though writing manual training loops brings all the flexibility to the user, it can also be error prone and require a lot of boilerplate code to make everything work. When this flexibility if not required, it is better to use a higher abstraction can be used.\n",
    "\n",
    "## Using Training Session\n",
    "\n",
    "Instead of writing the training loop manually and taking care of checkpointing and distribution herself, the user can delegate this aspects to the training session. It automatically takes care of the following things:\n",
    "    1. checkpointing\n",
    "    2. cross validation\n",
    "    3. testing/evaluation\n",
    "\n",
    "All that is needed from the user is to provide the corresponding configuration parameters. Plus to the higher abstraction the training session is also implemented in C++, so it is generally faster than writing a loop in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-25]: metric = 0.00% * 100;\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'training_session'\n",
    "\n",
    "# Minibatch sources\n",
    "mb_source = MinibatchSource(CTFDeserializer(input_file, streams))\n",
    "test_mb_source = MinibatchSource(CTFDeserializer(input_file, streams), randomize=False, max_samples=100)\n",
    "\n",
    "learner = cntk.distributed.data_parallel_distributed_learner(cntk.sgd(model.parameters, cntk.learning_rate_schedule(0.1, cntk.UnitType.sample)))\n",
    "trainer = cntk.train.Trainer(z, (loss, loss), learner, ProgressPrinter(freq=10))\n",
    "\n",
    "test_config=cntk.TestConfig(minibatch_source = test_mb_source,\n",
    "                            model_inputs_to_streams={ xy : test_mb_source['xy'], label : test_mb_source['r'] })\n",
    "\n",
    "session = cntk.training_session(\n",
    "    trainer = trainer, mb_source = mb_source, \n",
    "    mb_size = minibatch_size, \n",
    "    model_inputs_to_streams={ xy : mb_source['xy'], label : mb_source['r'] },\n",
    "    max_samples = max_samples,\n",
    "    checkpoint_config=cntk.CheckpointConfig(frequency=checkpoint_frequency, filename=checkpoint),\n",
    "    test_config=cntk.TestConfig(minibatch_source = test_mb_source, minibatch_size = minibatch_size,\n",
    "                                model_inputs_to_streams={ xy : test_mb_source['xy'], label : test_mb_source['r'] }))\n",
    "\n",
    "session.train()\n",
    "\n",
    "# When you use distributed learners, please call finalize MPI at the end of your script\n",
    "# cntk.distributed.Communicator.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to configure different aspects of the training session:\n",
    "\n",
    "### Progress tracking\n",
    "In order to report progress, the training session uses [Trainer.summarize_training_progress](https://www.cntk.ai/pythondocs/cntk.train.trainer.html?highlight=restore_from_checkpoint#cntk.train.trainer.Trainer.summarize_training_progress) after each progress_frequency samples (rounded to the border of minibatches). Implicitly this call is despatched to the corresponding calls of the [ProgressWriter](https://www.cntk.ai/pythondocs/cntk.logging.progress_print.html?highlight=progresswriter#module-cntk.logging.progress_print), which has its own set of parameters (i.e. freq can be used to specify how often to print loss value). ProgressWriter should be specified during trainer creation.\n",
    "If you need to have a custom logic for retrieving current status, please consider implementing your own ProgressWriter or using [Function.train](https://www.cntk.ai/pythondocs/cntk.ops.functions.html#cntk.ops.functions.Function.train) method.\n",
    "\n",
    "### Checkpointing\n",
    "[Checkpoint configuraiton](https://www.cntk.ai/pythondocs/cntk.train.training_session.html?highlight=checkpointconfig#cntk.train.training_session.CheckpointConfig) specifies how often to save a checkpoint to the given file. When given, the training session takes care of saving/restoring the state accross the trainer/learners/minibatch source and propogating this information among distributed workers. If you need to preserve all checkpoints that were taken during training, please set ```preserveAll``` to true. The checkpointing frequency is specified in samples.\n",
    "\n",
    "### Cross validation\n",
    "When [cross validation](https://www.cntk.ai/pythondocs/cntk.train.training_session.html?highlight=checkpointconfig#cntk.train.training_session.CrossValidationConfig) config is given, the training session runs the cross validation on the specified minibatch source with the specified frequency and reports average metric error. The user can also provide a cross validation callback, that will be called with the specified frequency. It is up to the user to perform cross validation in the callback and return back ```True``` if the training should be continued, or ```False``` otherwise. \n",
    "\n",
    "### Testing\n",
    "If the test configuration is given, after completion of the training the training session runs evaluation on the specified minibatch source. If you need to run only evaluation without training, consider using [Function.test](https://www.cntk.ai/pythondocs/cntk.ops.functions.html#cntk.ops.functions.Function.test) method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
